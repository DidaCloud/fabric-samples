{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b647fc18-77a4-4c04-b616-5f1c09d4bb69",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### **Evaluation Setup**:\n",
    "* Pip Install Fabric Data Agent SDK\n",
    "* Load the **DataFrame** with question and expected_answers list.\n",
    "  * You can update in-cell DataFrame.\n",
    "  * Or upload the CSV file in \"question,expected_answer\" format to lakehouse\n",
    "    * Copy the file path and load the data to DataFrame using pandas.read_csv(\"<lakehouse_filepath>\")\n",
    "* Invoke the evaluate_data_agent API with data_frame, **data_agent_name**, workspace_name (Optional), table_name (Optional).\n",
    "  * data_agent_name : Name of the Data Agent\n",
    "  * workspace_name (Optional) : Workspace Name if Data Agent is in different workspace. Default value is None.\n",
    "  * table_name (Optional) : Evaluation output table name to store the evaluation result. Default table name is 'evaluation_output'.\n",
    "    * After evaluation there will be two tables one with provided <table_name> for evaluation output and other with <table_name>_steps for detailed steps.\n",
    "  * data_agent_stage (Optional) : Data Agent stage i.e., sandbox or production. Default value is production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7700a-a21e-447f-ae95-71a77fe957c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Install Fabric Data Agent SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7ad166-a410-44ef-abf5-d6707cfa10b6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "%pip install -U fabric-data-agent-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc9434-4ffe-4c0d-a8d0-208f21d043e2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Load the Dataframe using in cell initialization or input csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0d0eb9-22c4-435d-a5f7-1fe9d0ba9513",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame with \"question,expected_answer\". Please update the questions and expected_answers as per the requirement.\n",
    "df = pd.DataFrame(columns=[\"question\", \"expected_answer\"],\n",
    "                  data=[\n",
    "                    [\"show total sales for Canadian Dollar for January 2013\", \"46,117.30.\"],\n",
    "                    [\"what is the product with the highest total sales for Canadian Dollar in 2013\", \"Mountain-200 Black, 42\"],\n",
    "                    [\"Total sales outside of US\", \"19968887.95\"],\n",
    "                    [\"which product category had the highest total sales for Canadian Dollar in 2013\", \"Bikes (Total Sales: 938654.76)\"]\n",
    "                ])\n",
    "\n",
    "# Load from input CSV file with data in format \"question,expected_answer\"\n",
    "# input_file_path = \"abfss://AgentEvaluation@dxt-onelake.dfs.fabric.microsoft.com/KaggleDataSetsLH.Lakehouse/Files/datasets/GeoNuclearData.csv\"\n",
    "# df = pd.read_csv(input_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7271b2-4722-4989-888b-daa65db1a122",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Invoke Evaluation API with input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023e6270-6a32-4501-85d9-4f1d322c37a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "from fabric.dataagent.evaluation import evaluate_data_agent\n",
    "\n",
    "# Data Agent name\n",
    "data_agent_name = \"AgentEvaluation\"\n",
    "\n",
    "# Workspace Name (Optional) if Data Agent is in different workspace\n",
    "workspace_name = None\n",
    "\n",
    "# Table name (Optional) to store the evaluation result. Default value is 'evaluation_output'\n",
    "# After evaluation there will be two tables one with provided <table_name> for evaluation output and other with <table_name>_steps for detailed steps.\n",
    "table_name = \"demo_evaluation_output_report\"\n",
    "\n",
    "# Data Agent stage ie., sandbox or production. Default to production.\n",
    "data_agent_stage = \"production\"\n",
    "\n",
    "# Evaluate the Data Agent. Returns the unique id for the evaluation run\n",
    "evaluation_id = evaluate_data_agent(df, data_agent_name, workspace_name=workspace_name, table_name=table_name, data_agent_stage=data_agent_stage)\n",
    "\n",
    "print(f\"Unique Id for the current evaluation run: {evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4372032-08b5-46ef-9e70-d43c79f794f3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Overall summary of an evaluation stored in the input table.\n",
    "Returns the DataFrame with summary details.\n",
    "\n",
    "Input Parameters:\n",
    "* table_name (Optional) : Table name which contains the evaluation result. Default value is 'evaluation_output'\n",
    "* verbose (Optional) : Flag to display the summary. Default is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c86294-5e64-40ab-96b2-fd956d8bf1e2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "from fabric.dataagent.evaluation import get_evaluation_summary\n",
    "\n",
    "# Table name (Optional) to store the evaluation result. Default value is 'evaluation_output'\n",
    "# After evaluation there will be two tables one with provided <table_name> for evaluation output and other with <table_name>_steps for detailed steps.\n",
    "table_name = \"demo_evaluation_output_report\"\n",
    "\n",
    "get_evaluation_summary(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d70a78-984c-4247-895b-572d27772eff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Evaluation details of a single run\n",
    "Returns the DataFrame with evaluation details.\n",
    "\n",
    "Input Parameters:\n",
    "* evaluation_id : Unique Id for an evaluation run.\n",
    "* table_name (Optional) : Table name which contains the evaluation result. Default value is 'evaluation_output'.\n",
    "* get_all_rows (Optional) : Flag to get all the rows for an evaluation. Default value is False, which returns only failed evaluation rows.\n",
    "* Verbose (Optional) : Flag to display the summary. Default is False.\n",
    "\n",
    "**Note**: The thread url in the evaluation details is only accessible by person who ran the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5319df98-bb8c-4382-8dd7-678c5f6f8b13",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "from fabric.dataagent.evaluation import get_evaluation_details\n",
    "\n",
    "# Unique Id for an evaluation run\n",
    "evaluation_id = 'd1621f67-7948-4b24-8d6b-79aa8ebf4464'\n",
    "# Evaluation output table name\n",
    "table_name = \"demo_evaluation_output_report\"\n",
    "# Flag to get all the rows for an evaluation. Default value is False, which returns only failed evaluation rows.\n",
    "get_all_rows = False\n",
    "# Flag to display the summary. Default is False.\n",
    "verbose = True\n",
    "\n",
    "eval_details = get_evaluation_details(evaluation_id, table_name, get_all_rows=get_all_rows, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2f33c-7637-4dce-9989-b44004dc90b8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Advanced Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11247b85-5475-4662-a693-50f9a04b616a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Use customized prompt for evaluation\n",
    "* critic_prompt (Optional): Prompt (Optional) to evaluate the actual answer from Data Agent. \n",
    "  * Please use the variables **query, expected_answer and actual_answer** as placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c1000c4-b24b-4fbb-9fb6-4bfc01c1ec3f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "from fabric.dataagent.evaluation import evaluate_data_agent\n",
    "\n",
    "# Prompt (Optional) to evaluate the actual response. Please use the varaibles query, expected_answer and actual_answer as placeholders\n",
    "critic_prompt = \"\"\"\n",
    "        Given the following query, expected answer, and actual answer, please determine if the actual answer is equivalent to expected answer. If they are equivalent, respond with 'yes'.\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Expected Answer:\n",
    "        {expected_answer}\n",
    "\n",
    "        Actual Answer:\n",
    "        {actual_answer}\n",
    "\n",
    "        Is the actual answer equivalent to the expected answer?\n",
    "        \"\"\"\n",
    "\n",
    "# Data Agent name\n",
    "data_agent_name = \"AgentEvaluation\"\n",
    "\n",
    "# Evaluate the Data Agent. Returns the unique id for the evaluation run\n",
    "evaluation_id = evaluate_data_agent(df, data_agent_name, critic_prompt=critic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f4d000-753a-4467-8b58-3dc8dde10a08",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "source": [
    "from fabric.dataagent.evaluation import get_evaluation_details\n",
    "\n",
    "# Unique Id for an evaluation run\n",
    "evaluation_id = '4e725e05-5b72-493f-b849-d8787decc188'\n",
    "# Evaluation output table name\n",
    "table_name = \"evaluation_output\"\n",
    "# Flag to get all the rows for an evaluation. Default value is False, which returns only failed evaluation rows.\n",
    "get_all_rows = False\n",
    "# Flag to display the summary. Default is False.\n",
    "verbose = True\n",
    "\n",
    "eval_details = get_evaluation_details(evaluation_id, table_name, get_all_rows=get_all_rows, verbose=verbose)"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "40957e0d-5221-4ada-857f-4dd6bbedd539",
    "default_lakehouse_name": "EvaluationDemoLH",
    "default_lakehouse_workspace_id": "3515fff7-6395-4b60-a5ba-52977e32f9c6",
    "known_lakehouses": [
     {
      "id": "40957e0d-5221-4ada-857f-4dd6bbedd539"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
